{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nX= train_data[[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex\",\"Embarked\"]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n                               ('scaler',StandardScaler())\n                                       ])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),\n        ('cat', categorical_transformer, [\"Sex\",\"Embarked\",\"Pclass\"])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import cross_val_score\n\n# scores =  cross_val_score(my_pipeline, X, y,\n#                               cv=5,\n#                               scoring=\"accuracy\")\n\n# print(\"Accuracy scores:\\n\", scores)\n# print(\"Average accuracy score (across experiments):\")\n# print(scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(X, y, model, folds, metric):\n\t# create the pipeline\n\tmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\t#evaluate model\n\tscores = cross_val_score(my_pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n\treturn scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate a model and try to trap errors and and hide warnings\ndef robust_evaluate_model(X, y, model, folds, metric):\n\tscores = None\n\ttry:\n\t\twith warnings.catch_warnings():\n\t\t\twarnings.filterwarnings(\"ignore\")\n\t\t\tscores = evaluate_model(X, y, model, folds, metric)\n\texcept:\n\t\tscores = None\n\treturn scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_models(X, y, models, folds=10, metric='accuracy'):\n\tresults = dict()\n\tfor name, model in models.items():\n\t\t# evaluate the model\n\t\tscores = robust_evaluate_model(X, y, model, folds, metric)\n\t\t# show process\n\t\tif scores is not None:\n\t\t\t# store a result\n\t\t\tresults[name] = scores\n\t\t\tmean_score, std_score = mean(scores), std(scores)\n\t\t\tprint('>%s: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n\t\telse:\n\t\t\tprint('>%s: error' % name)\n\treturn results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary classification spot check script\nimport warnings\nfrom numpy import mean\nfrom numpy import std\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_models(models=dict()):\n\t# linear models\n\tmodels['logistic'] = LogisticRegression()\n\talpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\tfor a in alpha:\n\t\tmodels['ridge-'+str(a)] = RidgeClassifier(alpha=a)\n\tmodels['sgd'] = SGDClassifier(max_iter=1000, tol=1e-3)\n\tmodels['pa'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n\t# non-linear models\n\tn_neighbors = range(1, 21)\n\tfor k in n_neighbors:\n\t\tmodels['knn-'+str(k)] = KNeighborsClassifier(n_neighbors=k)\n\tmodels['cart'] = DecisionTreeClassifier()\n\tmodels['extra'] = ExtraTreeClassifier()\n\tmodels['svml'] = SVC(kernel='linear')\n\tmodels['svmp'] = SVC(kernel='poly')\n\tc_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\tfor c in c_values:\n\t\tmodels['svmr'+str(c)] = SVC(C=c)\n\tmodels['bayes'] = GaussianNB()\n\t# ensemble models\n\tn_trees = 100\n\tmodels['ada'] = AdaBoostClassifier(n_estimators=n_trees)\n\tmodels['bag'] = BaggingClassifier(n_estimators=n_trees)\n\tmodels['rf'] = RandomForestClassifier(n_estimators=n_trees)\n\tmodels['et'] = ExtraTreesClassifier(n_estimators=n_trees)\n\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=n_trees)\n\tmodels['XGBoost']=XGBClassifier(n_estimators=n_trees)\n\tmodels['Catboost']=CatBoostClassifier(n_estimators=n_trees)\n\tprint('Defined %d models' % len(models))\n\treturn models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = define_models()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = evaluate_models(X, y, models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print and plot the top n results\ndef summarize_results(results, maximize=True, top_n=10):\n\t# check for no results\n\tif len(results) == 0:\n\t\tprint('no results')\n\t\treturn\n\t# determine how many results to summarize\n\tn = min(top_n, len(results))\n\t# create a list of (name, mean(scores)) tuples\n\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n\t# sort tuples by mean score\n\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n\t# reverse for descending order (e.g. for accuracy)\n\tif maximize:\n\t\tmean_scores = list(reversed(mean_scores))\n\t# retrieve the top n for summarization\n\tnames = [x[0] for x in mean_scores[:n]]\n\tscores = [results[x[0]] for x in mean_scores[:n]]\n\t# print the top n\n\tprint()\n\tfor i in range(n):\n\t\tname = names[i]\n\t\tmean_score, std_score = mean(results[name]), std(results[name])\n\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n\t# boxplot for the top n\n\tpyplot.boxplot(scores, labels=names)\n\t_, labels = pyplot.xticks()\n\tpyplot.setp(labels, rotation=90)\n\tpyplot.savefig('spotcheck.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summarize_results(results)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}